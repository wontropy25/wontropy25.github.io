---
title: "4. Support Vector Machine"
date: 2024-09-25 00:01:00 +0900
categories: [학부 수업, (3-2) Machine Learning]
tags: [machine learning]     # TAG names should always be lowercase
math: true
mermaid: true
---

### [4.1] Support Vector Machine
---
- 데이터들을 선형 분류를 하고 싶은데 아래와 같은 2차원 XOR 문제의 경우는 아무리 직선을 그어도 선형 분류가 불가능하다. 이와 같은 문제를 **비선형 분류 문제**라고 한다.

![4-1](assets/img/school_ml/ml4-1.png){: style="margin-left: auto; margin-right: auto; width: 50%;"}

- 이 경우 우리는 **Support Vector Machine**을 이용하면 선형 분류로 변환할 수 있다. 위의 데이터를 적절히 변환하면 선형 분류할 수 있는 직선을 찾을 수 있다.
- 선형 분류 문제는 Decision Tree, Neural Network, SVM의 성능이 모두 동일하다.
- 비선형 분류 문제는 Neural Network가 SVM보다 성능이 더 좋거나 같다.
- **비선형 분류 문제를 고차원에 사상하여 선형 분류로 만들 수 있으면 SVM이 다른 모델보다 훨씬 유리하다!**

> **Support Vector Machine** <br/>
데이터들을 kernel function을 이용해 고차원 공간으로 사상시킨 후, support vector들로 이루어진 초평면을 이용하여 선형 분류하는 margin 기반의 기계학습 모델
{: .prompt-info}

#### Kernel Function
- 저차원 데이터를 고차원 공간으로 사상(Mapping)시키는 함수
- 선형 분리가 불가능한 문제를 선형 분리가 가능한 문제로 변환시키는 함수이다.
- 예를 들어 위의 XOR 문제의 경우 $\varphi(x) = x^2$ 함수로 데이터를 변환하면 $2$차원 공간에서 선형분류를 할 수 있게 된다.

#### Support Vectors

![4-2](assets/img/school_ml/ml4-2.png){: style="margin-left: auto; margin-right: auto; width: 50%;"}

- **Support Vectors**: 선형 분류 형태로 데이터가 주어졌을 때 경계 주변에 존재하는 데이터 포인트들을 의미한다.
- **Maximum margin**: 선형 분류를 가능하게 하는 직선들 중 Margin이 최대가 되도록 하는 직선이 가장 이상적인 직선이다.
- SVM의 목적: support vector 사이의 margin이 최대가 되도록 하는 직선 $Wx+b$를 찾기! ($3$차원 이상의 공간일 경우 $W$가 벡터가 되서 초평면을 찾아야 한다.)

#### Maximum Margin
먼저 $2$차원의 예시에서 Maximum Margin을 통해 문제의 방향성을 잡아보자.
- 문제의 조건: 데이터들이 $(x_i, y_i)$ 형태로 주어진다. 
  - $x_i$는 $n$차원 벡터이고 정답이 $1$인 데이터를 $x_+$, 정답이 $-1$인 데이터를 $x_{-}$로 쓴다. 
  - $y_i$는 정답 데이터로 $1, -1$의 값을 갖는다.
- 목적: 데이터들의 Margin을 최대로 하는 직선을 찾는 것!

우선 직선 $wx+b$를 찾기 위해 기울기 $w$를 고정해보자. 직선을 위아래로 움직이면서 정답이 $1$인 데이터들의 Support Vector와 만나는 직선을 찾아보면 식 $\ref{1}$, 정답이 $-1$인 데이터들의 Support Vector와 만나는 직선을 찾아보면 식 $\ref{2}$가 나온다.

$$ \begin{equation}wx_+ + b = 1 \label{1}\end{equation}$$ 
$$ \begin{equation}wx_{-}+ b = -1 \label{2}\end{equation}$$ 

위의 두 직선 사이의 거리를 margin이라고 한다. 서포트 벡터에 대한 margin 값을 구하면 $ M = \frac{2}{\vert\vert\mathbf{w}\vert\vert} $ 이다. 즉, margin을 최대화하는 것은 $\vert\vert\mathbf{w}\vert\vert$을 최소화하는 것과 같다.  

### [4.2] Optimization Problem
---
주어진 원래 문제는 다음과 같이 제약조건 하에서 목적함수의 최솟값을 찾는 것이였다.
- 목적함수: $Q(\mathbf{w}) = \frac{1}{2}\vert\vert\mathbf{w}\vert\vert^2$ 
- 제약조건: $y_i (\mathbf{w} \cdot \mathbf{x}_i - b) \geq 1,\ \forall (\mathbf{x}_i, y_i) \in D$

#### Lagrange Multiplier Method
주어진 제약조건 하에서 목적함수의 최솟값을 구하는 문제는 **Lagrange Multiplier Method (라그랑주 승수법)** 으로 나타낼 수 있다.

- Lagrange Function은 다음과 같이 나타낸다. 목적함수를 $f(x, y)$, 제약조건을 $S$라 하면 목적함수에서 제약조건의 상수배를 뺀 것을 의미한다.

$$ J(x, y, \alpha) = f(x, y) - \alpha S $$

- 우리의 문제를 Lagrange Function으로 나타내면 다음과 같다. 


$$
\begin{equation} J(\mathbf{w}, b, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^{n} \alpha_i \left( y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right) \end{equation}
$$

- 원래 목표는 목적함수의 최솟값을 구하는 것이였지만, 이제는 제약조건을 포함한 라그랑주 함수의 최솟값을 구하는 것이 목표이다.
- $w$와 $b$에 대한 최솟값을 구해야 한다. 따라서 위의 식의 미지수인 $\mathbf{w}, b$에 대해서 각각 미분한 값이 $0$인 지점을 찾으면 된다. 

$$
\begin{equation} \frac{\partial J(\mathbf{w}, b, \alpha)}{\partial \mathbf{w}} = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i \end{equation}
$$

$$
\begin{equation} \frac{\partial J(\mathbf{w}, b, \alpha)}{\partial b} = 0 \quad \Rightarrow \quad \sum_{i=1}^{n} \alpha_i y_i = 0 \end{equation}
$$

#### Dual Problem
위의 식 $(4), (5)$를 이용해 식 $(3)$을 정리해보면 $\mathbf{w}, b$의 값이 사라지고 $\alpha$만 남게 된다.

$$
\begin{equation}
 J(\mathbf{w}, b, \alpha) = \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j
 \end{equation}
$$

원래 목적은 목적 함수의 최솟값을 구하는 것이였는데 이제 위의 식에서 $\mathbf{x}_i$와 $y_i$의 값은 모두 데이터를 대입하면 되므로 $\alpha$에 대해 정리된다. 따라서 원래 문제를 아래와 같이 $\alpha$의 최댓값을 구하는 새로운 문제로 바꿀 수 있다. 이를 **Dual Problem** 이라고 한다.

- 목적함수: $Q(\alpha) = \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j$
- 제약조건: $\sum_{i} \alpha_i y_i = 0, \quad \alpha \geq 0$

위의 식을 통해 얻은 결과는 다음과 같다.

$$
\begin{equation}
F(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} - b \quad \Rightarrow \quad F(\mathbf{x}) = \sum_{i} \alpha_i y_i \mathbf{x}_i \cdot \mathbf{x} - b
\end{equation}
$$

### [4.3] Kernel Trick
---
실제로는 SVM 함수는 고차원 공간으로 사상을 한 후 내적을 통해서 구현이 된다. 

- 위의 $(7)$번 식에서 실제 SVM은 $\varphi(\mathbf{x}_i) \cdot \varphi(\mathbf{x})$ 으로 커널함수를 통과시킨 값을 내적한다.

그러나, 차원을 변경하지 않고 **현재 차원에서 동일한 효과를 거두는 커널 함수**가 존재한다. 이를 사용하여 고차원인 것 처럼 트릭을 주는 것을 **Kernel Trick** 이라고 한다.

- Kernel Trick 함수의 조건: $K(u, v) = \varphi(u) \cdot \varphi(v)$ 
- 벡터 $u, v$에 대하여 고차원으로 사상했을 때 내적한 값과 커널함수의 값이 같아야 한다!
- Polynomial 커널 함수: $K(a, b) = (a \cdot b + 1)^d$ (보통 $d=2$)

### SVM 파이썬 실습
---
```python
import numpy as np

file_path = "SMSSpamCollection.dat"

# 파일 읽기
x_data, y_data = [], []

# 한 줄씩 끊어서 라인으로 넣음
with open(file_path,'r',encoding='utf8') as inFile:
  lines = inFile.readlines()

# 데이터 읽기 (처음부터 100개까지만)
lines = lines[:100]

# line 하나씩 가져오기
for line in lines:
  line = line.strip().split('\t') 
  sentence, label = line[1], line[0]
  x_data.append(sentence)
  y_data.append(label)
```
- 데이터의 형태(SMSSpamCollection)는 다음과 같다.
  - `ham\tGo until jurong point, crazy..`
  - `spam\tPlease call 07046744435 now to arrange delivery`
- 입력 데이터를 읽고 \t을 기준으로 입력 문장을 분리한 후에 문장과 라벨을 각각 `x_data, y_data` 리스트에 저장
- `strip()`: 눈에 안보이는 공백 제거 
- `split('\t')`: tab으로 앞뒤 자르기

#### 문자열 데이터 숫자로 변환
```python
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()

# spam, ham 라벨을 대응하는 index로 치환하기위한 딕셔너리
label2index_dict = {'spam':0, 'ham':1}

# indexing 한 데이터를 넣을 리스트 선언
indexing_x_data, indexing_y_data = [], []

for label in y_data:
  indexing_y_data.append(label2index_dict[label])

# x_data를 사용하여 딕셔너리 생성
tokenizer.fit_on_texts(x_data)

# x_data에 있는 각 문장의 단어들을 대응하는 index로 치환하고 그 결과값을 indexing_x_data에 저장
indexing_x_data = tokenizer.texts_to_sequences(x_data)

print("x_data indexing 하기 전 : " + str(x_data[0]))
print("x_data indexing 하기 후 : " + str(indexing_x_data[0]))
print("y_data indexing 하기 전 : " + str(y_data[0]))
print("y_data indexing 하기 후 : " + str(indexing_y_data[0]))
```
-  `tokenizer.fit_on_texts(data)`: 각 단어를 index로 치환하기 위한 딕셔너리를 생성. 생성된 딕셔너리는 tokenizer 객체 안에 저장된다.
- 예시: `{'to': 1, 'i': 2, 'you': 3, 'a': 4, 'the': 5, 'and': 6, 'for': 7 ... }`
- `tokenizer.texts_to_sequences(data)`: 문장 안에 있는 단어들을 딕셔너리를 이용해 index로 치환

```text
x_data indexing 하기 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...
x_data indexing 하기 후 : [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127]
y_data indexing 하기 전 : ham
y_data indexing 하기 후 : 1
```

#### SVM 모델 학습
```python
from sklearn.svm import SVC

max_length = 60
for index in range(len(indexing_x_data)):
  length = len(indexing_x_data[index])

  if(length > max_length):
    indexing_x_data[index] = indexing_x_data[index][:max_length] 
  elif(length < max_length):
    indexing_x_data[index] = indexing_x_data[index] + [0]*(max_length-length)

# 전체 데이터를 9:1의 비율로 나누어 학습 및 평가 데이터로 사용
number_of_train = int(len(indexing_x_data)*0.9)

train_x = indexing_x_data[:number_of_train]
train_y = indexing_y_data[:number_of_train]
test_x = indexing_x_data[number_of_train:]
test_y = indexing_y_data[number_of_train:]

print("train_x의 개수 : " + str(len(train_x)))
print("train_y의 개수 : " + str(len(train_y)))
print("test_x의 개수 : " + str(len(test_x)))
print("test_y의 개수 : " + str(len(test_y)))

svm = SVC(kernel='linear', C=1e10)
svm.fit(train_x, train_y)
```

- 데이터의 문장 길이를 모두 고정된 길이 $60$으로 변환하였다. (긴 부분은 잘라내고 짧은 부분은 $0$으로 채우기)
- 전체 데이터를 9:1의 비율로 나누어 학습 및 평가 데이터로 사용
- `kernel = linear`: 커널 트릭 함수로 linear 함수를 사용
- `C=1e10`: Slack 변수로 margin 사이에 오류 값을 얼마나 허용할지에 대한 값
- `svm.fit`: svm으로 훈련하는 함수

#### SVM 모델 평가
```python
predict = svm.predict(test_x)

correct_count = 0
for index in range(len(predict)):
  if(test_y[index] == predict[index]):
    correct_count += 1

accuracy = 100.0*correct_count/len(test_y)

print("Accuracy: " + str(accuracy))

# 숫자를 문자열로 다시 변경
index2label = {0:"spam", 1:"ham"}

test_x_word = tokenizer.sequences_to_texts(test_x)

for index in range(len(test_x_word)):
  print()
  print("문장 : ", test_x_word[index])
  print("정답 : ", index2label[test_y[index]])
  print("모델 출력 : ", index2label[predict[index]])
```
- `svm.predict(data)`: SVM 모델의 평가
- 원래는 ham, spam에 대한 accuracy, recall을 각각 구해서 $F_1-score$를 구해야 하지만 여기서는 두 비율이 비슷하기 때문에 최종 accuracy로 구하였다.
- 최종 결과는 아래와 같이 나온다.

```text
Accuracy: 80.0

문장 :  yeah do don‘t stand to close tho you‘ll catch something
정답 :  ham
모델 출력 :  spam

문장 :  sorry to be a pain is it ok if we meet another night i spent late afternoon in casualty and that means i haven't done any of y stuff42moro and that includes all my time sheets and that sorry
정답 :  ham
모델 출력 :  spam

문장 :  smile in pleasure smile in pain smile when trouble pours like rain smile when sum1 hurts u smile becoz someone still loves to see u smiling
정답 :  ham
모델 출력 :  ham

문장 :  please call our customer service representative on 0800 169 6031 between 10am 9pm as you have won a guaranteed ￡1000 cash or ￡5000 prize
정답 :  spam
모델 출력 :  spam

문장 :  havent planning to buy later i check already lido only got 530 show in e afternoon u finish work already
정답 :  ham
모델 출력 :  ham
```