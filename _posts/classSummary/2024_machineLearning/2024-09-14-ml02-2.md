---
title: "[기계학습] 2-1. An overview of gradient descent optimization algorithms"
date: 2024-09-14 00:01:00 +0900
categories: [학부 수업, (3-2) Machine Learning]
tags: [machine learning]     # TAG names should always be lowercase
math: true
mermaid: true
---

> 논문 출처: **[An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747)**

## 1. Introduction
---
- 경사 하강법은 목적함수 $J(\theta)$를 최소화하기 위한 기법이다. ($d$개의 feature를 가진다고 가정한다. 즉, $\theta \in \mathbb{R}^d$)
- 매개변수 $\theta$를 목적 함수의 기울기 $\nabla_\theta J(\theta)$의 반대 방향으로 업데이트 하는 것이 목표이다. 
- 학습률 $\eta$은 local minimum에 도달하기 위한 스텝(epoch)의 크기를 결정한다. 

## 2. Gradient descent variants
---
얼마나 많은 데이터를 사용하느냐에 따라서 경사하강법의 종류를  3가지로 분류할 수 있다. 데이터 양에 따라 매개변수 업데이트의 정확도와 업데이트 수행에 걸리는 시간 사이에서 trade-off가 발생한다.

### Batch gradient descent (배치 경사 하강법)

$$
\begin{equation}\theta = \theta - \eta \cdot \nabla_{\theta} J(\theta)\end{equation}
$$

- 모든 데이터를 한번에 다 쓰는 방식으로, 우리가 익히 알고 있는 일반적인 경사 하강법이 배치 경사 하강법이다.
- 한번의 업데이트를 위해 모든 데이터를 사용하기 때문에, 속도가 매우 느리고 메모리에 맞지 않는 데이터 셋을 다루기 힘들 수 있다.
- 새로운 데이터를 실시간으로 추가하여 모델을 온라인으로 업데이트하는 것을 허용하지 않는다.
- convex surfaces 에서는 global minimum을 찾고, non-convex surfaces 에서는 local minimum을 찾는 것을 보장한다.

```python
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params) 
  params = params - learning_rate * params_grad
```

### Stochastic gradient descent (확률적 경사 하강법)

$$
\begin{equation}
\theta = \theta - \eta \cdot \nabla_{\theta} J(\theta; x^{(i)}, y^{(i)})
\end{equation}$$

- $\text{BGD}$의 문제는 한 번 업데이트를 하기 위해 전체 데이터셋을 돌아야 해서 속도가 느리다는 것이였다.
- $\text{SGD}$는 하나의 데이터셋 $x^{(i)};y^{(i)}$ 으로만 학습을 진행한다. 따라서 속도가 더 빠르고, 온라인 업데이트에도 적합하다.
- $\text{SGD}$는 자주 업데이트를 하기 때문에 분산이 높아져서 목적 함수의 fluctuation(변동성)이 높아질 수 있다. 변동성 덕분에 더 나은 local minimum을 찾을 수도 있지만, 정확한 최솟값으로 수렴하지 않고 과하게 움직일 수도 있다.
- 매 epoch 마다 훈련 데이터를 섞어서 반복해야 한다.

```python
for i in range(nb_epochs): 
  np.random.shuffle(data) 
  for example in data:
    params_grad = evaluate_gradient(loss_function , example , params) 
    params = params - learning_rate * params_grad
```

### Mini-batch gradient descent (미니 배치 경사 하강법)

$$
\begin{equation}
\theta = \theta - \eta \cdot \nabla_{\theta} J(\theta; x^{(i:i+n)}, y^{(i:i+n)})
\end{equation}$$

- 위의 두 가지 방법의 장점만을 합쳐서 만든 것으로, 매번 $n$개의 미니 배치로 훈련을 한다.
- 매개변수 업데이트의 분산을 줄여 더 안정적인 수렴을 가능하게 하고, 최적화된 행렬 연산을 사용하여 미니 배치에 대한 기울기 계산을 효율적으로 수행할 수 있다.
- 보통 미니 배치의 크기는 $50 \sim 256$ 정도이고, $\text{SGD}$를 이야기할 때 보통 이 방법을 많이 지칭한다.

```python
for i in range(nb_epochs): 
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params) 
    params = params - learning_rate * params_grad
```

## 3. Challenges
---
1. 적절한 학습률을 선택하는 것은 쉽지 않은 일이다. 학습률이 너무 작으면 수렴 속도가 매우 느려지고, 반대로 너무 크면 수렴이 방해받거나 손실 함수가 최소값 주변에서 요동치거나 심지어 발산할 수 있다.

2. 이를 해결하기 위해 Learning rate schedule이 사용되는데, 이는 epoch 간 목표 변화가 일정 임계값 아래로 떨어질 때 미리 정해진 방식으로 학습률을 줄여주는 방법이다. 하지만 이러한 스케줄과 임계값은 사전에 정의되어야 하며, 데이터셋의 특성에 맞춰 유연하게 대응하기 어렵다.

3. 모든 매개변수 업데이트에 동일한 학습률을 적용하는 것도 문제가 될 수 있다. 데이터가 희소하고 특성 간 발생 빈도가 크게 다를 경우, 드물게 나타나는 특성에 대해서는 더 큰 업데이트가 필요할 수 있기 때문이다.

4. 신경망에서 비볼록 오류 함수를 최소화할 때 또 다른 어려움은 비최적의 지역 최소값에 갇히는 것이다. 그러나 Dauphin 등은 문제의 핵심이 지역 최소값이 아니라 saddle points(안장점)에 있다고 주장한다. 안장점은 한 차원에서는 기울기가 올라가고 다른 차원에서는 내려가는 지점으로, 동일한 오류를 가진 고원으로 둘러싸여 있어 기울기가 거의 0에 가깝다. 이 때문에 SGD로는 이러한 지점에서 벗어나기가 매우 어렵다.