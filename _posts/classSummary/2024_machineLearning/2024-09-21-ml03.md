---
title: "3. Decision Tree"
date: 2024-09-21 00:01:00 +0900
categories: [학부 수업, (3-2) Machine Learning]
tags: [machine learning]     # TAG names should always be lowercase
math: true
mermaid: true
---

### [3.1] Information Gain
---
- **Decision Tree**: Feature들의 Information gain에 따라 트리 형태의 규칙을 자동 생성하는 기계학습 모델. 이 글에서는 날씨, 온도, 습도, 바람에 따라 테니스 시합을 하는지 안 하는지를 결정하는 문제를 다룬다.
- **Feature**: 문제 해결에 영향을 미치는 관측 및 측정 가능한 요소 (outlook, temp, humidity, wind)
- **Entropy**: 문제의 복잡도
- **Information Gain**: Feature의 값을 알게 됨으로써 얻어지는 엔트로피의 감소 기대치

#### Entropy
정보 이론에서 엔트로피는 **특정 확률 $p$를 가진 메시지를 송신하는데 필요한 비트 수에 대한 기댓값**을 의미한다. 일반적으로 정보 엔트로피는 다음과 같이 쓴다.

$$ H(X) = -\sum_{x \in X} p(x) \log p(x)$$

어떤 표본공간 $S$ 내에서 Positive 결과의 비율을 $p_{(+)}$라 하고 Negative 결과의 비율을 $p_{(-)}$라 하면 $S$의 엔트로피를 다음과 같이 나타낸다.

$$ Entropy(S) = -p_{(+)} \log_2p_{(+)} -p_{(-)} \log_2p_{(-)} $$

- 확률이 한쪽으로 몰려있을수록 엔트로피가 낮다.
- 확률이 $1$이거나 $0$이면 엔트로피가 $0$이다.
- 확률이 $\frac{1}{2}$로 가장 애매할 때 엔트로피가 최댓값을 가진다. 

#### Information Gain
Feature의 값을 알게 됨으로써 얻어지는 엔트로피의 감소 기대치

$$ IG(S, A) = Entropy(S) - \sum_{v \in V_{values(A)}} \frac{|S_v|}{|S|} Entropy(S_v)$$

- 감소 기대치가 커질수록 엔트로피를 많이 낮출 수 있다. 즉, 문제의 복잡도를 낮출 수 있다.
- 정보 획득량 = 원래 엔트로피 - 자질을 알게되었을 때의 엔트로피

### [3.2] Decision Tree for Play Tennis
---

$$
\begin{array}{|c|c|c|c|c|c|}
\hline
\textbf{Day} & \textbf{Outlook} & \textbf{Temperature} & \textbf{Humidity} & \textbf{Wind} & \textbf{PlayTennis} \\ \hline
D1 & \text{Sunny} & \text{Hot} & \text{High} & \text{Weak} & \text{No} \\ \hline
D2 & \text{Sunny} & \text{Hot} & \text{High} & \text{Strong} & \text{No} \\ \hline
D3 & \text{Overcast} & \text{Hot} & \text{High} & \text{Weak} & \text{Yes} \\ \hline
D4 & \text{Rain} & \text{Mild} & \text{High} & \text{Weak} & \text{Yes} \\ \hline
D5 & \text{Rain} & \text{Cool} & \text{Normal} & \text{Weak} & \text{Yes} \\ \hline
D6 & \text{Rain} & \text{Cool} & \text{Normal} & \text{Strong} & \text{No} \\ \hline
D7 & \text{Overcast} & \text{Cool} & \text{Normal} & \text{Strong} & \text{Yes} \\ \hline
D8 & \text{Sunny} & \text{Mild} & \text{High} & \text{Weak} & \text{No} \\ \hline
D9 & \text{Sunny} & \text{Cool} & \text{Normal} & \text{Weak} & \text{Yes} \\ \hline
D10 & \text{Rain} & \text{Mild} & \text{Normal} & \text{Weak} & \text{Yes} \\ \hline
D11 & \text{Sunny} & \text{Mild} & \text{Normal} & \text{Strong} & \text{Yes} \\ \hline
D12 & \text{Overcast} & \text{Mild} & \text{High} & \text{Strong} & \text{Yes} \\ \hline
D13 & \text{Overcast} & \text{Hot} & \text{Normal} & \text{Weak} & \text{Yes} \\ \hline
D14 & \text{Rain} & \text{Mild} & \text{High} & \text{Strong} & \text{No} \\ \hline
\end{array}
$$

위의 데이터를 가지고 Decision Tree를 만드는 방법은 Information Gain이 높은 자질 순으로 정렬해서 노드를 결정하면 된다. 따라서 먼저 각 Feature 별로 Information Gain을 계산해보자.

- 초기 엔트로피: 정답 Column만 보고 엔트로피를 구한다. 초기 표본공간을 $S$라 하면 $S=[9+, 5-]$ 이므로 positive 결과가 $\frac{9}{14}$, negative 결과가 $\frac{5}{14}$ 이다.

$$Entropy(S) = -\frac{9}{14}\log_2 \frac{9}{14} - \frac{5}{14} \log_2\frac{5}{14} = 0.94$$

- 각 Feature을 알게되었을 때의 Information Gain을 각각 계산한다.

#### Information Gain
Wind의 feature value는 Weak 또는 Strong 두 가지이다. 따라서 초기 엔트로피에서 두 개의 표본공간으로 쪼개졌을 때 각각의 엔트로피의 기댓값의 합만큼을 뺀 값이 정보 획득량이다. 각각의 기댓값은 그 표본공간이 전체 공간에서 차지하는 비율과 그 표본공간의 엔트로피의 곱으로 표현된다.

$$ IG(S, Wind) = Entropy(S) - \frac{|S_{Weak}|}{|S|} Entropy(S_{Weak}) -  \frac{|S_{Strong}|}{|S|} Entropy(S_{Strong})$$

- Weak 일 때 표본공간은 $S_{Weak} = [6+, 2-]$, Strong일 때 표본공간은 $S_{Strong} = [3+, 3-]$

$$ Entropy(S_{Weak}) = -\frac{6}{8}\log_2 \frac{6}{8} - \frac{2}{8} \log_2\frac{2}{8} = 0.811$$

$$ Entropy(S_{Strong}) = -\frac{3}{6}\log_2 \frac{3}{6} - \frac{3}{6} \log_2\frac{3}{6} = 1$$

$$ \therefore IG(S, Wind) = 0.94 - \frac{8}{14} Entropy(S_{Weak}) -  \frac{6}{14} Entropy(S_{Strong}) = 0.048$$

위와 같은 방식으로 outlook, humidity, temperature의 Information Gain을 다음과 같이 구한다.
- $IG(S, outlook) = 0.246$
- $IG(S, humidity) = 0.151$
- $IG(S, temperature) = 0.029$

Outlook 자질을 알게되었을 때 정보획득량이 가장 크므로 최우선 노드는 Outlook이 된다.

#### Deciding Second Node
![3-1](assets/img/school_ml/ml3-1.png){: style="margin-left: auto; margin-right: auto; width: 50%;"}
Outlook 으로 내려온 이후 각 표본공간마다 두번째 노드를 결정해보자.

> - Outlook 은 이미 학습에 완료한 Feature 이므로 더이상 고려하지 않는다.
- Outlook의 Value 별로 각각 트리가 형성되어 그 value가 적용된 표본공간만을 고려해야 한다. 즉, Sunny 공간의 경우 전체 행 중 Sunny가 적용된 행이 전체라고 가정하고 엔트로피를 계산해야 한다.
{: .prompt-tip}

$$
\begin{array}{|c|c|c|c|c|c|}
\hline
\textbf{Day} & \textbf{Outlook} & \textbf{Temperature} & \textbf{Humidity} & \textbf{Wind} & \textbf{PlayTennis} \\ \hline
D1 & \text{Sunny} & \text{Hot} & \text{High} & \text{Weak} & \text{No} \\ \hline
D2 & \text{Sunny} & \text{Hot} & \text{High} & \text{Strong} & \text{No} \\ \hline
D8 & \text{Sunny} & \text{Mild} & \text{High} & \text{Weak} & \text{No} \\ \hline
D9 & \text{Sunny} & \text{Cool} & \text{Normal} & \text{Weak} & \text{Yes} \\ \hline
D11 & \text{Sunny} & \text{Mild} & \text{Normal} & \text{Strong} & \text{Yes} \\ \hline
\end{array}
$$

##### 초기 엔트로피
Sunny일 때의 정답 column만 보고 엔트로피를 구한다. $S_{Sunny} = [2+, 3-]$이므로 positive 결과가 $\frac{2}{5}$, negative 결과가 $\frac{3}{5}$ 이다.

$$Entropy(S_{Sunny}) = -\frac{2}{5}\log_2 \frac{2}{5} - \frac{3}{5} \log_2\frac{3}{5} = 0.971$$

이제 outlook을 제외한 나머지 feature들의 Information Gain을 구한다. 이때 표본공간은 Sunny일 때만 봐야한다.

##### Temperature
- $S_{Hot} = [0+, 2-],\ S_{Mild} = [1+, 1-],\ S_{Cool} = [1+, 0-]$
- $Entropy(S_{hot}) = -\frac{0}{2}\log_2 \frac{0}{2} - \frac{2}{2}\log_2 \frac{2}{2} = 0$
- $Entropy(S_{Mild}) = -\frac{1}{2}\log_2 \frac{1}{2} - \frac{1}{2}\log_2 \frac{1}{2} = 1$
- $Entropy(S_{Cool}) = -\frac{1}{1}\log_2 \frac{1}{1} - \frac{0}{1}\log_2 \frac{0}{1} = 0$

> 결과가 $[0+, 2-],\ [1+, 0-]$ 처럼 한쪽으로 몰려있으면 엔트로피는 $0$이고, $[1+, 1-]$처럼 절반의 확률을 가지면 엔트로피는 $1$이다.
{: .prompt-tip}

$$IG(S_{Sunny}, temp) = 0.971 - \frac{2}{5} Entropy(S_{hot}) - \frac{2}{5} Entropy(S_{Mild}) - \frac{1}{5} Entropy(S_{Cool}) = 0.571$$

##### Humidity
- $S_{High} = [0+, 3-],\ S_{Normal} = [2+, 0-]$
- 이 경우 결과가 모두 한쪽으로 몰려있으므로 엔트로피가 $0$이다. 따라서 이 feature을 알게되면 엔트로피가 최대로 감소하므로 정보 획득량이 최대이다.

$$IG(S_{Sunny}, humidity) = 0.971 - 0 = 0.971$$

##### Wind
- $S_{Strong} = [1+, 1-],\ S_{Weak} = [1+, 2-]$
- $Entropy(S_{Strong}) = -\frac{1}{2}\log_2 \frac{1}{2} - \frac{1}{2}\log_2 \frac{1}{2} = 1$
- $Entropy(S_{Weak}) = -\frac{1}{3}\log_2 \frac{1}{3} - \frac{2}{3}\log_2 \frac{2}{3} = 0.918$

$$IG(S_{Sunny}, wind) = 0.971 - \frac{2}{5} Entropy(S_{Strong}) - \frac{3}{5} Entropy(S_{Weak}) = 0.020$$

##### Decision Tree
따라서 Sunny의 두번째 노드에는 Humidity가 와야한다. 이와 같은 방식으로 다른 Feature들에도 Information Gain을 구하여 엔트로피가 $0$이 될때까지 감소시키면 아래와 같은 트리가 결정된다.
![3-2](assets/img/school_ml/ml3-2.png){: style="margin-left: auto; margin-right: auto; width: 75%;"}

- Decision Tree를 생성하고 나면 Rule로 해석이 가능해진다. 따라서 사용자가 트리를 구성한 후에 임의로 변경하는 것이 편리하다.


### [3.3] Feature 구성 시 고려사항
---
1. **연속 자질 (Continuous Feature)**: 엔트로피를 측정하기 위해서는 이산 자질 (Discrete Feature)로 변환해야 한다. 예를 들어 기온이 $15$도 아래면 Cool, $25$도 아래면 Mild, 그 이상이면 Hot으로 변환한다.
2. **복잡한 자질**: 빈도가 낮아서 엔트로피가 매우 높게 측정되므로 독립적으로 작용 가능한 경우 분할해서 생각하는 것이 좋다. 예를 들어 Date 자료를 $2024-09-22$와 같이 사용하면 이 feature value의 값은 $365 \times 2024$개가 존재할 수 있으므로 feature로서의 효용성이 매우 낮아진다. 따라서 Year, Month, Day로 나눠서 feature을 만드는 것이 좋다.
3. **비싼 자질**: 측정/관측 비용을 엔트로피에 반영할 수 있다. 예를 들어 피검사는 $2$만원인데 MRI 검사는 $50$만원이 든다고 하면 엔트로피 계산 시 MRI 검사 결과에 가중치를 더 부여하는 것을 의미한다.

### [3.4] Decision Tree의 장단점
---
- **Feature Selection**: 엔트로피를 계산해서 feature가 중요한지 안 중요한지를 자동으로 정해준다.
- **Handles features of different types**: feature 간의 타입이 달라도 트리 생성을 할 수 있다.
- **Fast Prediction**: 트리를 생성한 후 결과를 도출하는 과정이 매우 빠르다. ($O(\log n)$)
- **Interpretable decision rules**: 트리를 If문을 사용한 rule로 변경하기가 쉽기 때문에 분석이 쉽고 사용자가 조작하기에 용이하다.

---

- 각 feature value 들이 결합되지 않고, 독립적으로 다뤄진다. 따라서 **feature간의 의존 관계**를 반영하는 것이 불가능하다.
  - 이에 대한 해결책으로 $(A, B), (B, C), (A, C), (A, B, C)$ 와 같이 세 feature들의 쌍을 feature로 만들어서 사용하는 것이 가능하다.
- Optimal tree를 찾는 것이 어렵다.

### 의사 결정 트리 파이썬 실습
---
이번 실습의 목표는 위에서 다룬 테니스 예제의 의사 결정 트리를 직접 파이썬으로 구현하는 것이다.

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn import tree
import graphviz

# 데이터 경로
file_path = "PlayTennis.csv"

# 데이터 경로로부터 파일을 읽음 (pandas 라이브러리 사용)
datas = pd.read_csv(file_path)

# 범주형 데이터를 수치형 데이터로 자동 변환해주는 라이브러리
label_encoder = LabelEncoder()

# 정답 클래스 이름 (yes, no)
target_names = label_encoder.fit(datas['play']).classes_
print("target_names : {}".format(target_names))

datas['outlook'] = label_encoder.fit_transform(datas['outlook'])
datas['temp'] = label_encoder.fit_transform(datas['temp'])
datas['humidity'] = label_encoder.fit_transform(datas['humidity'])
datas['windy'] = label_encoder.fit_transform(datas['windy'])
datas['week'] = label_encoder.fit_transform(datas['week'])
datas['time'] = label_encoder.fit_transform(datas['time'])
datas['play'] = label_encoder.fit_transform(datas['play'])

# 데이터 출력 (데이터 포맷 변환 후 결과 확인)
print(datas)
```

```text
     outlook  temp humidity  windy play
0      sunny   hot     high  False   no
1      sunny   hot     high   True   no
2   overcast   hot     high  False  yes
3      rainy  mild     high  False  yes
4      rainy  cool   normal  False  yes
5      rainy  cool   normal   True   no
6   overcast  cool   normal   True  yes
7      sunny  mild     high  False   no
8      sunny  cool   normal  False  yes
9      rainy  mild   normal  False  yes
10     sunny  mild   normal   True  yes
11  overcast  mild     high   True  yes
12  overcast   hot   normal  False  yes
13     rainy  mild     high   True   no
```
- `label_encoder.fit(datas['play']).classes_`를 통해 해당 열에 들어있는 feature에 어떤 value들이 무엇인지 집합으로 알 수 있다.
- `label_encoder.fit_transform(datas['outlook'])`은 범주형 데이터를 수치형 데이터로 변환하는 함수이다. 예를 들어 outlook의 overcast는 $0$, rainy는 $1$, sunny는 $2$와 같이 변환한다.

```python
# 입력 데이터와 정답 데이터로 분리
x_data, y_data = datas.drop(['play'], axis=1), datas['play']

# Decision tree 모델 학습 ()
decision_tree = tree.DecisionTreeClassifier(criterion = 'entropy')
train_result = decision_tree.fit(x_data, y_data)

# 학습 결과 확인 (graphviz 라이브러리 사용)
target_names = [str(name) for name in target_names]

graph = graphviz.Source(tree.export_graphviz(train_result, out_file=None,
                                             feature_names=x_data.columns,
                                             class_names=target_names))
graph
```
- 입력 데이터는 정답 열인 `play` 열을 제외한 열들이므로 `datas.drop(['play'], axis=1)`로 나타내고, 정답 데이터는 정답 열인 `play`열만 포함한다.
- `tree.DecisionTreeClassifier(criterion = 'entropy')`, `decision_tree.fit(x_data, y_data)`를 통해 의사 결정 트리를 생성한다. 

![3-3](assets/img/school_ml/ml3-3.png){: style="margin-left: auto; margin-right: auto; width: 75%;"}

```python
# 학습한 모델을 사용하여 예측 (Closed test)
predict_result = decision_tree.predict(x_data) 

# 예측 결과 출력 (실제 정답을 맞춘 경우 True로 표시됨)
print(predict_result == y_data)
```

```text
0     True
1     True
2     True
3     True
4     True
5     True
6     True
7     True
8     True
9     True
10    True
11    True
12    True
13    True
Name: play, dtype: bool
```

- `decision_tree.predict(x_data)`를 통해 실제 데이터를 집어 넣어서 예측이 맞는지 확인한다. 
- 코드에서는 훈련에 사용한 `x_data` 데이터를 가지고 Closed 테스트를 했으므로 당연히 다 맞추게 된다.