---
title: "1. Data Mining Process"
date: 2024-09-21 00:01:00 +0900
categories: [학부 수업, (3-2) Data Science]
tags: [data science]     # TAG names should always be lowercase
math: true
mermaid: true
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---

### [1.1] Data Mining Processing
---
1. Define, understand purpose 
2. Obtain Data
  - 결과가 일반화될 수 있도록 다양한 데이터 입력을 샘플링해줘야 한다. 
  - ex) 과금 유저와 무과금 유저의 데이터 비율을 다르게 적용
3. Explore, Clean, Preprocess Data 
  - ex. 사람 몸무게가 500kg이 들어왔다면 정리해야함
4. **Reduce the Data**
  - 데이터 차원 축소: 연속적인 데이터들을 카테고리화 하는 것, 데이터 연산 (AND, OR) 등을 통해 새로운 변수 생성
5. **Specify Task**
  - 클러스터링을 할건지, 분류를 할건지 등등 
6. **Choose The Techniques**
  - 어떤 데이터 마이닝 기술을 쓸 것인가? 접근 방법에 관한 것
7. Iterative Implementation and Tuning
  - 유효한 데이터를 가지고 과정을 반복
8. Assess Results
  - 평가 데이터(Test Data set)는 훈련 때 사용하면 안됨! 
9. Deploy the Best model


#### Terminology
![ds1-3](assets/img/school_ds/ds1-3.png)

- Data Object: 릴레이션의 Row 속성으로, 데이터 객체 하나를 칭한다. 
- Attribute: 릴레이션의 Column 속성으로, 데이터 객체의 특성을 의미한다.
- Data Set: 데이터 객체들의 집합
- Explanatory attribute: 설명 속성으로, 입력 변수들의 속성을 의미한다. (=Independent variable, Predicator)
- Target attribute: 최종적으로 구하는 속성을 의미한다. (=Dependent variable)

### [1.2] Data Types
---

![ds1-1](assets/img/school_ds/ds1-1.png)

#### Categorical (Qualitative) Attribute
범주형 자료는 텍스트나 숫자 형태로 표시될 수 있다. 이때 숫자는 주민등록번호처럼 대수로서의 의미를 가지지 않기 때문에 대수연산이 불가능하다.

- **norminal** 명목형 자료: 순서가 없는 자료. ex) 남자, 여자 등
- **ordinal** 순위형 자료: 순서가 있는 자료. ex) low, mid, high
- **binary** 자료: Pass or Fail, Yes or No

#### Numerical (Quantitative) Attribute
대수적 성질을 가지고, 측정 단위가 존재할 수 있다.

- **discrete** 변수
- **continuous** 변수

#### Record (Table) Data
- Data Set은 레코드의 집합이고, 각 레코드는 고정된 attribute들을 가진다.

![ds1-2](assets/img/school_ds/ds1-2.png)

#### Graph Data
- **데이터 사이의 연관성을 쉽게 분석**할 수 있게 만들어준다.

1. 데이터 객체들 간의 관계를 나타내는 경우: 노드는 데이터 객체이고 링크가 각 관계에 해당한다. (ex. SNS 네트워크)
2. 데이터 자체가 그래프 형태인 경우: 벤젠 분자와 같이 그 구조 자체가 그래프일 수도 있다.

> **그래프 데이터와 테이블 데이터의 차이** <br/>
- 문서를 인용한 문서를 재귀적으로 탐색하고자 할 경우, 테이블 데이터는 조인을 많이 해야 하지만 그래프는 DFS, BFS 방식을 통해 손쉽게 탐색이 가능하다.
- 새로운 속성을 추가해야 하는 경우, 테이블 데이터는 테이블을 새로 정의해야 하는 반면 그래프는 단순히 노드를 추가해주는 방식으로 다룰 수 있다.
{: .prompt-tip}

#### Sequential Data
1. Sequential transaction data: 'A를 구매한 후 B를 구매했다'와 같이 시간순으로 일어난 일 (Symbolic한 데이터 시퀀스)
2. Time series data: 주식과 같이 수치적인 데이터가 연속적으로 움직이는 경향
3. Sequence data: A, T, G, C와 같이 연속적으로 구성된 데이터

#### 데이터의 특성
- Dimensionality: Column이 너무 많으면 차원의 저주에 빠져 패턴 학습이 어려워진다. 따라서 차원 축소를 자주 사용한다.
- Distribution: 이상적으로는 정규분포가 되길 원하지만, 실제 데이터 세트는 한쪽으로 치우치는 등 다른 분포를 지닌다.


#### Data Selecting
```python
import pandas as pd

# 파일 읽기
housing_df = pd.read_csv('WestRoxbury.csv')

housing_df.shape # (5802, 14) - 행렬의 모양
housing_df.head() # 첫 5개 행만 보여줌

# 속성값 스페이스 공백을 _으로 변경
housing_df.columns = [s.strip().replace(' ', '_') for s in housing_df.columns]
```

![ds1-5](assets/img/school_ds/ds1-5.png)

#### Selection & Slicing
```python
# loc vs. iloc
housing_df.loc[0:3]  # 0행부터 3행 포함
housing_df.loc[0:3, 'TOTAL_VALUE']
housing_df.iloc[0:4] # 0행부터 4행전까지 (즉, 0행부터 3행까지)

# 2차원 배열
housing_df['TOTAL_VALUE'].iloc[0:10]
housing_df.iloc[0:10]['TOTAL_VALUE']
housing_df.iloc[0:10].TOTAL_VALUE
housing_df['TOTAL_VALUE'][0:10]

# 연속되지 않은 두 열을 합쳐 새로운 데이터 프레임 구성
pd.concat([housing_df.iloc[4:6,0:2], housing_df.iloc[4:6,4:6]], axis=1)

# full column은 :으로 지칭
housing_df.iloc[:,0:1]
housing_df.TOTAL_VALUE

# 평균보다 높은 값들을 가진 값만을 마스킹하겠다.
mask = housing_df['TOTAL_VALUE'] > housing_df['TOTAL_VALUE'].mean()
housing_df.loc[mask, :]

# 통계정보
print('Number of rows ', len(housing_df['TOTAL_VALUE'])) # show length of first column
print('Mean of TOTAL_VALUE ', housing_df['TOTAL_VALUE'].mean()) # show mean of column
housing_df.describe() # show summary statistics for each column
```
- full column 전체를 지칭하려면 `[:]`와 같이 사용하면 된다. `[:,:]`는 이차원 행렬 기준 전체를 지칭한다.
- `concat(~, axis=1)`: 두 데이터 프레임을 하나로 합친다. axis가 $1$이면 배열의 $2$번째 단계를 기준으로 합치겠다, 즉 열 기준으로 합치는 것을 의미한다.

### [1.3] Data Quality
---
실제 데이터들은 완벽하지 않고, Missing된 정보가 있거나 여러가지 요인에 의해 이상한 정보가 담겨 있을 수 있다. (ex. 키가 2m이고 몸무게가 2kg인 사람)
- Measurement and Data collection error: 데이터 수집 시 발생하는 오류. 결측치 등이 해당한다.
- Noise: 랜덤하게 발생하는 측정 오류. **제거하기가 어렵기 때문에 Robust한 알고리즘으로 데이터를 다뤄야 한다!** 

#### Bias-Variance Tradeoff
![ds1-4](assets/img/school_ds/ds1-4.png)

- Precision (정밀도, 분산에 반비례)
- Bias (편향)

1. 분산이 낮고 편향이 낮은 것이 가장 이상적인 형태이다.
2. **Overfitting**: 분산이 높고 편향이 낮은 것. 모델의 복잡도가 높아 학습 데이터에 대해 너무 학습이 잘되서 실제 데이터에 대해서는 오차가 크게 나타난다. (족보만 믿고 공부하다가 새로운 문제가 나왔을 때 풀지 못하는 것과 비슷하다.)
3. **Underfitting**: 분산이 낮고 편향이 높은 것. 모델이 너무 간소화되어 예측 결과가 잘 맞지 않고 다른 데이터를 넣어도 비슷하게 결과가 나온다. 

#### IQR, Outliers
- IQR: 데이터들이 주어졌을 때 $25%$에서 $75%$ 사이의 영역을 의미한다. (즉 4분위 값 중 1분위 ~ 3분위 사이)
- IQR의 1.5배를 넘어가는 영역을 Outlier로 구분한다.

#### Handling Missing Data
결측치가 적은 경우 nan, null 등으로 처리하거나 삭제할수도 있다. 또는 중위값이나 평균값 등으로 대체해서 넣어줄수도 있다.
```python
# nan 값으로 처리
missingRows = housing_df.sample(10).index
housing_df.loc[missingRows, 'BEDROOMS'] = np.nan

# 결측치가 있는 행 삭제
reduced_df = housing_df.dropna()

# 중위값으로 대체
medianBedrooms = housing_df['BEDROOMS'].median()
housing_df.BEDROOMS = housing_df.BEDROOMS.fillna(value=medianBedrooms)
```

#### Inconsistent Values and Duplicate Data
```python
# 이상한 데이터가 있는지 마스킹으로 확인
# 'FLOORS': 0 or too high values
housing_df[(housing_df['FLOORS'] <= 0) | (housing_df['FLOORS'] > 5)]

# 'TOTAL VALUE': negative or extremely high values
housing_df[(housing_df['TOTAL_VALUE'] <= 0) | (housing_df['TOTAL_VALUE'] > 10000)]

# 'REMODEL': inconsistent casing
housing_df[~housing_df['REMODEL'].isin(['None', 'Recent'])]

# 중복 데이터 파악
housing_df_sub = housing_df[['TAX', 'LOT_SQFT', 'YR_BUILT']]

# 세가지 값이 모두 동일한 것들을 추출
housing_df_sub[housing_df_sub.duplicated(keep=False)]

# 첫번째만 살리고 그 이후의 중복들은 다 삭제
housing_df_sub.drop_duplicates(keep='first')
```

### [1.4] Data Preprocessing
---
- Aggregation: `GROUP BY` 등을 통해 여러 데이터들을 묶어서 하나의 객체로 만드는 것
- Sampling: 집단을 대표할 수 있는(Representative) 샘플을 뽑아 데이터의 부분집합을 사용하는 것
- Binarization: 속성을 $0$ 또는 $1$의 이진 값만을 바꾸게 하는 것
- Discretization: 연속된 데이터를 이산적인 데이터 형태로 바꾸는 것
- Variable transformation: 변수의 분포를 바꾸는 것

#### Aggregation
- 작은 데이터일수록 메모리와 프로세싱 시간을 적게 차지하므로 데이터 개수를 줄이는 것이 좋다.
- 그룹으로 데이터를 분석하는 것이 각각을 분석할 때보다 더 안정적으로 움직인다.
```python
# Average TOTAL VALUE and TAX by number of BEDROOMS
housing_df.groupby('BEDROOMS'). agg({
    'TOTAL_VALUE': 'mean',
    'TAX': 'mean'
}).reset_index()
```

#### Sampling
집단을 대표할 수 있는(Representative) 샘플을 뽑아 데이터의 부분집합을 사용하는 것
- Simple Random Sampling: 완전 랜덤 선택, 뽑은 것을 다시 집어 넣을지 말지에 따른 선택이 존재
- Stratified Sampling: 클래스의 개수를 고려해서 비율 맞춰서 뽑기
- Progressive Sampling: 정확도를 고려하여 샘플 사이즈를 정하기 

```python
# Random Sampling
# Randomly selects 10 samples from the dataset.
housing_df.sample(n=10, random_state=2024)

# Random Sampling
# Selects every 5th row from the dataset 
housing_df.iloc[::5]

# Stratified Sampling
# Groups the data by the number of bedrooms (BEDROOMS) and takes up to 5 samples from each group.
housing_df.groupby('BEDROOMS', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))
```

#### Binarization
Binarzation은 Gender와 같이 $0$ 또는 $1$로 표현 가능한 데이터로 변환하는 것을 의미한다.

- One hot encoding: $m$개의 카테고리 값이 있으면 각각이 있는 자리를 $1$로 갖고 나머지는 다 $0$으로 채우는 방식을 의미한다.
- **Multicollinearity (다중공선성 문제)**: 모델의 변수들끼리 강한 상관관계를 가지는 문제. 위의 방식을 사용하면 입력 변수들끼리 상관관계가 생겨버려서 다중공선성 문제가 발생한다.
- 해결법: $n$개의 카테고리가 있으면 $n-1$개의 변수만을 사용하면 된다. 즉, 아무 더미 변수(reference category) 하나를 삭제해주면 해결할 수 있다.

```python
# because of Multicollinearity
pd.get_dummies(data['REMODEL'], prefix='REMODEL', prefix_sep='_', dummy_na=True, drop_first=True)
```

#### Discretization
Discretization은 Temparature와 같은 연속적인 데이터를 Hot, Cold 등과 같은 이산적 데이터로 변환하는 것을 의미한다. 

![ds1-6](assets/img/school_ds/ds1-6.png)

1. Equal width 이산화: 모두 같은 길이로 잘라서 그룹화를 하는 것. 위의 데이터 형태에서는 적합하지 않다.
2. Equal frequency 이산화: 각 구간마다 같은 개수가 존재하도록 빈도수를 통해 그룹화를 하는 것. 위의 데이터 형태에서는 적합하지 않다.
3. Clustering-based 이산화: 유의미한 것들끼리 그룹화를 하는 것. 위의 데이터 형태에서 $k=4$로 지정해주면 적합하다.

```py
from sklearn.preprocessing import KBinsDiscretizer

data = housing_df.copy()

total_value = data['TOTAL_VALUE'].values.reshape(-1, 1)

discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans', subsample=None)
binned_total_value = discretizer.fit_transform(total_value)

label_mapping = {0: 'low', 1: 'medium', 2: 'high'}
data['TOTAL_VALUE (Discretized)'] = pd.Series(binned_total_value.ravel()).replace(label_mapping)
data[['TOTAL_VALUE', 'TOTAL_VALUE (Discretized)']]
```

#### Variable Transformation
데이터의 분포를 잘 맞춰주기 위해서 Scaling하는 과정을 의미한다. 단순히 $\sqrt x, \log x$ 연산을 취해서 정규분포 형태로 만들어주기도 하고, 코사인 유사도나 유클리드 거리 등을 통해 스케일링을 해주기도 한다.

- Z-score 정규화: 우리가 아는 그 정규분포화. 처음부터 잘 정제된 데이터가 들어오면 Z-score 방식이 유리하다.

$$ Z = \frac{X-\mu}{\sigma} $$

- min-max 정규화: 최솟값 $0$에서 최댓값 $1$사이의 값으로 분포화. Outlier 영역의 값에 영향을 크게 받을 수 있다는 단점이 있다. (e.g. $1$부터 $100$짜리 데이터 세트가 있는데 하나만 $100000$이면 $1$부터 $100$사이 값들이 거의 무시된다.)

$$
X_{\text{min-max}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
$$

- Robust 정규화: 중위값과 IQR를 사용하여 스케일링 하기 때문에 outlier가 있어도 영향을 덜 받는다.

$$
X_{\text{robust}} = \frac{X-\text{median}(X)}{\text{IQR}(X)}
$$

### [1.5] Data Visualization
---
- 시각화의 목적: 데이터 탐색과 예비 분석을 위해 plot으로 표현
- 파이썬 라이브러리: Matplotlib, pandas, plotly 등

#### Basic Plots
- Line Plot: 시간에 따른 변화를 나타냄
- Bar Chart: 카테고리 변수와 숫자형 변수의 관계를 나타냄
- Scatter Plot: 두 숫자형 변수 사이의 관계를 나타냄

#### Distribution Plots
- Histograms: 각 value가 얼마나 등장하는지를 나타냄
- Box Plot: Outlier와 max, min 값을 명시할 때 사용
- Heat Maps: 변수간의 상관관계를 시각화할 때 사용 (Missingness를 측정할 때 유용하다.)

![ds1-7](assets/img/school_ds/ds1-7.png){: style="margin-left: auto; margin-right: auto; width: 50%;"}

#### Multidimensional Visualization
Matrix Scatterplot은 상관계수를 함께 표기하고 분포도 같이 보여준다. 따라서 heat map보다 분포 자체를 한눈에 파악하기가 쉽다.

![ds1-8](assets/img/school_ds/ds1-8.png){: style="margin-left: auto; margin-right: auto; width: 50%;"}

#### Specialized Visualization
- Network Data Visualization: 노드와 엣지를 사용해 관계를 표현
- Hierarchical Data Visualization (Treemaps): 다양한 차원의 데이터 탐색 가능
- Geographical Data Visualization (Map Charts): 실제 지도에 표현