---
title: "[기계학습] 2. Hypothesis, Loss function"
date: 2024-09-10 00:01:00 +0900
categories: [학부 수업, (3-2) Machine Learning]
tags: [machine learning]     # TAG names should always be lowercase
math: true
mermaid: true
---

### [2.1] Linear Hypothesis
---
Learning (학습) 이란 무엇일까? 다음과 같은 데이터셋이 있다고 가정하자.

$$ (x,\ y) = (1,\ 1), (2,\ 2), (3,\ 3)$$

우리는 $x = 4$일 때 $y$값이 무엇일 것 같냐고 물으면 당연히 $4$라고 할 것이다. 그러나 기존의 컴퓨터 소프트웨어는 이런 일을 할 수가 없었다. 그럼 우리는 왜 $4$를 넣으면 $4$가 나온다고 생각할 수 있을까?

#### Hypothesis
우리는 머릿속에서 점들을 잘 나타내는 직선의 방정식($H(x)=x$)을 찾아야 되겠다고 가정을 했기 때문에 찾을 수 있었다. 이와 같은 가설을 $\text{Linear Hypothesis}$ 라고 부른다.
- $\textbf{Hypothesis function}: H(x) = Wx + b$

우리는 데이터셋에서 $(x, y)$ 의 순서쌍들을 알고 있다. 이를 통해 $H(x)$의 $W,\ b$ 값을 찾아내는 것이 문제이다.

#### Matrix Representation
위에서 데이터셋의 feature는 총 1개밖에 존재하지 않았다. ($x$값 하나가 $y$값을 결정했기 때문) 그러나 실제로 데이터셋은 feature의 개수가 수백 수천개씩 존재할 수 있다. 데이터셋의 feature 크기가 $n$이라고 한다면 $(x_i, y_i)$ 순서쌍의 개수가 총 $n$개 존재하여 $n$차원의 공간에서 데이터를 표현해야 한다. 따라서 $H(x)$ 함수를 다음과 같이 작성할 수 있다.

$$H(x_1, x_2, x_3, \dots, x_n) = w_1x_1 + w_2x_2 + w_3x_3 + \dots + w_nx_n + b$$


입력값 $x_1, x_2, \dots x_n$들을 벡터 $X$로 나타내고, 구해야하는 값 $w_1, w_2, \dots w_n$을 벡터 $W$로 나타내자. 위의 식을 다시 행렬로 표현하면 식이 더 깔끔해진다.

$$
H(X) = 
\begin{bmatrix}
  b & w_1 & w_2 & w_3
\end{bmatrix}
\times
\begin{bmatrix}
    1 \\
    x_1 \\
    x_2 \\
    x_3
\end{bmatrix}
=
\begin{bmatrix}
    b \times 1 + w_1 \times x_1 + w_2 \times x_2 + w_3 \times x_3
\end{bmatrix}
= W^T X
$$

위의 식 표현 외에도 $H(X) = WX$로 Transpose 표시를 제외하고 나타내기도 하고, $H(X) = WX + b$로 $b$ 벡터와 함께 표시하기도 한다. 이 모든 표현들은 다 똑같은 의미로 받아들이자. 결국 셋 다 $W,\ b$를 찾는 것이 목표이다.

### [2.2] Cost function
---
위에서 우리의 목표는 데이터셋들에 가장 가까운 직선(의 기울기 $W$ 와 절편 $b$)을 찾는 것이였다. 학습의 첫 시작 시 $W$ 값은 랜덤하게 선택된다. 기울기가 $W$, 절편이 $b$인 직선과 데이터셋 사이의 오차를 $\textbf{cost function}: cost(W, b)$ 이라 한다.

오차를 단순히 두 결과의 차이 ($H(x) - y$)로만 계산을 하면 $+$ 오차와 $-$ 오차가 서로 상쇄될 수 있다. 그럼 단순히 절댓값을 씌우면 되는게 아닌가 싶은데 절댓값 함수는 미분이 불가능하기 때문에 나중에 문제가 생긴다. 그래서 오차에 제곱을 한 $\textbf{MSE(Mean Square Error)}$ 값을 사용해야 한다.

$$cost = \frac{1}{m} \sum_{i=1}^{m} \left( H(x^{(i)}) - y^{(i)} \right)^2$$

예를 들어, $W = 2, b = 0$인 직선과 데이터셋 $$ (x,\ y) = (1,\ 1), (2,\ 2), (3,\ 3)$$의 오차는 다음과 같다. $H(x) = 2x$ 이므로

$$ \frac{((2 \times 1) - 1)^2 + ((2 \times 2) - 2)^2 + ((2 \times 3) - 3)^2}{3}$$

#### Object function

작성 예정