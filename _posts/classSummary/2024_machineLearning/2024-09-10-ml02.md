---
title: "[기계학습] 2. Gradient descent algorithm"
date: 2024-09-10 00:01:00 +0900
categories: [학부 수업, (3-2) Machine Learning]
tags: [machine learning]     # TAG names should always be lowercase
math: true
mermaid: true
---

### [2.1] Linear Hypothesis
---
Learning (학습) 이란 무엇일까? 다음과 같은 데이터셋이 있다고 가정하자.

$$ (x,\ y) = (1,\ 1), (2,\ 2), (3,\ 3)$$

우리는 $x = 4$일 때 $y$값이 무엇일 것 같냐고 물으면 당연히 $4$라고 할 것이다. 그러나 기존의 컴퓨터 소프트웨어는 이런 일을 할 수가 없었다. 그럼 우리는 왜 $4$를 넣으면 $4$가 나온다고 생각할 수 있을까?

#### Hypothesis
우리는 머릿속에서 점들을 잘 나타내는 직선의 방정식($H(x)=x$)을 찾아야 되겠다고 가정을 했기 때문에 찾을 수 있었다. 이와 같은 가설을 $\textbf{Linear Hypothesis}$ 라고 부른다.
- $\textcolor{red}{\textbf{Hypothesis function}}: H(x) = Wx + b$

우리는 데이터셋에서 $(x, y)$ 의 순서쌍들을 알고 있다. 이를 통해 $H(x)$의 $W,\ b$ 값을 찾아내는 것이 문제이다.

#### Matrix Representation
위에서 데이터셋의 feature는 총 1개밖에 존재하지 않았다. ($x$값 하나가 $y$값을 결정했기 때문) 그러나 실제로 데이터셋은 feature의 개수가 수백 수천개씩 존재할 수 있다. 데이터셋의 feature 크기가 $n$이라고 한다면 $(x_i, y_i)$ 순서쌍의 개수가 총 $n$개 존재하여 $n$차원의 공간에서 데이터를 표현해야 한다. 따라서 $H(x)$ 함수를 다음과 같이 작성할 수 있다.

$$\begin{equation}\label{eq1} H(x_1, x_2, x_3, \dots, x_n) = w_1x_1 + w_2x_2 + w_3x_3 + \dots + w_nx_n + b\end{equation}$$

입력값 $x_1, x_2, \dots x_n$들을 벡터 $X$로 나타내고, 구해야하는 값 $w_1, w_2, \dots w_n$을 벡터 $W$로 나타내자. 위의 식을 다시 행렬로 표현하면 식이 더 깔끔해진다.

$$
\begin{equation}\label{eq2} 
H(X) = 
\begin{bmatrix}
  b & w_1 & w_2 & w_3
\end{bmatrix}
\times
\begin{bmatrix}
    1 \\
    x_1 \\
    x_2 \\
    x_3
\end{bmatrix}
=
\begin{bmatrix}
    b \times 1 + w_1 \times x_1 + w_2 \times x_2 + w_3 \times x_3
\end{bmatrix}
= W^T X
\end{equation}
$$

위의 식 표현 외에도 $H(X) = WX$로 Transpose 표시를 제외하고 나타내기도 하고, $H(X) = WX + b$로 $b$ 벡터와 함께 표시하기도 한다. 이 모든 표현들은 다 똑같은 의미로 받아들이자. 결국 셋 다 $W,\ b$를 찾는 것이 목표이다.

### [2.2] Cost function
---
위에서 우리의 목표는 데이터셋들에 가장 가까운 직선(의 기울기 $W$ 와 절편 $b$)을 찾는 것이였다. 학습의 첫 시작 시 $W$ 값은 랜덤하게 선택된다. 기울기가 $W$, 절편이 $b$인 직선과 데이터셋 사이의 오차를 $\textcolor{red}{\textbf{cost function}}: cost(W, b)$ 이라 한다.

오차를 단순히 두 결과의 차이 ($H(x) - y$)로만 계산을 하면 $+$ 오차와 $-$ 오차가 서로 상쇄될 수 있다. 그럼 단순히 절댓값을 씌우면 되는게 아닌가 싶은데 절댓값 함수는 미분이 불가능하기 때문에 나중에 문제가 생긴다. 그래서 오차에 제곱을 한 $\textcolor{red}{\textbf{MSE(Mean Square Error)}}$ 값을 사용해야 한다.

$$\begin{equation}\label{eq3} cost = \frac{1}{m} \sum_{i=1}^{m} \left( H(x^{(i)}) - y^{(i)} \right)^2 \end{equation}$$

최종 목표는 오차 $cost(W, b)$ 값을 가장 작게 하는 $W$와 $b$를 찾는 것이다. 이를 찾기 위한 함수를 목적함수 $\textbf{object function}: \underset{W, b}{\min} cost(W, b)$ 라고 한다.

#### Cost function example

예를 들어, $W = 2, b = 0$인 직선과 데이터셋 $$ (x,\ y) = (1,\ 1), (2,\ 2), (3,\ 3)$$의 오차는 다음과 같다. $H(x) = 2x$ 이므로

$$ cost =  \frac{((2 \times 1) - 1)^2 + ((2 \times 2) - 2)^2 + ((2 \times 3) - 3)^2}{3} = 4.67$$

만약 $W = 1, b = 0$이였으면 오차 값은 $0$이므로 정답을 완벽하게 찾은 것이다. 물론 실제 경우에서 오차가 $0$이 되는 정답을 찾는 것은 거의 불가능하고, 이는 데이터셋이 한 직선 위에 있기 때문에 가능한 것이다.

### [2.3] Gradient Descent
---

![2-1](assets/img/school_ml/ml2-1.png){: .w-50 .right} 


$$cost = \frac{1}{m} \sum_{i=1}^{m} \left( H(x^{(i)}) - y^{(i)} \right)^2$$

$W$ 값에 따라 $cost$ 값이 변하므로 다음과 같이 $W$값을 $x$축, $cost$값을 $y$축으로 갖는 그래프를 그려보자. 이와 같은 볼록 함수를 $\textbf{convex function}$ 라고 한다. 볼록 함수에서는 항상 $cost$ 값의 최솟값 또는 최댓값을 찾을 수 있다. 오른쪽 그래프에서 $cost$값이 최소가 되도록 하는 $W$ 값은 $1$이다.

이제 학습을 통해 $W$ 값을 찾는 과정을 알아보자. $cost$ 값을 줄이는 방향으로 가야 한다는 것을 생각하자.
1. 처음 $W$ 값은 랜덤하게 주어진다.
  - 만약 $W$가 정답보다 작은 경우, 기울기가 $(-)$ 이므로 오른쪽$(+)$으로 이동해야 한다.
  - 만약 $W$가 정답보다 큰 경우, 기울기가 $(+)$ 이므로 왼쪽$(-)$으로 이동해야 한다.
2. 기울기를 하강하면서 정답에 다가가야 한다. 따라서 학습율 $\alpha$를 곱해서 정답에 수렴하도록 만든다. 이 값이 없으면 기울기가 수렴하지 않고 좌우로만 움직인다. 그렇다고 이 값이 너무 커지면 정답을 놓치고 넘어갈수도 있기 때문에 적당히 $0.005 \sim 0.01$ 정도의 값을 사용한다.

#### Formal Definition of Gradient Decent
1. $$cost(W) = \frac{1}{2m} \sum_{i=1}^{m} \left( H(x^{(i)}) - y^{(i)} \right)^2$$ (미분했을 때 계산을 편하게 하기 위해 2를 나눠줌)
2. $$ \Delta W = \alpha \frac{\partial}{\partial W} cost(W)$$ : 다음 단계의 $W$로 가기 위해 변화하는 값
3. 기울기의 반대 방향으로 움직여야 하므로 기존 $W$로부터 $\Delta W$만큼을 **빼줘야** 한다. 따라서 $$ W := W - \alpha \frac{\partial}{\partial W} \frac{1}{2m} \sum_{i=1}^{m} \left( W x^{(i)} - y^{(i)} \right)^2$$
4. $W$에 대해 편미분을 진행하면 아래와 같이 식이 정리된다.

$$
\begin{equation}\therefore W := W - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( W x^{(i)} - y^{(i)} \right) x^{(i)}\end{equation}
$$

> **Q. 그냥 미분해서 0인 점을 구하면 되는거 아닌가요? 왜 경사하강법을 통해서 $W$를 찾아야 하나요?**
- 1,2차원의 경우에는 0인 점을 찾아서 쉽게 W를 결정할 수 있지만, 실제 우리가 풀고자 하는 문제는 매우 큰 차원의 W이고 학습데이터의 양도 매우 크기 때문에 해당 방정식을 푸는 것보다 경사하강법을 이용하여 찾는 것이 더 효율적이다.
- 분류 문제를 풀 때 사용하는 비선형 함수들은 미분 계수와 그 근을 계산하기 어려운 경우가 많다.
- 우리가 풀고자 하는 문제의 비용함수는 local minimum을 많이 가진 형태로 만들어진다. 아래에서 다룰 B.C.E의 경우에 log를 씌우는 방법으로 local minimum을 줄이려고 하지만 완벽한 convex 함수는 아니기 때문에 이런 경우 미분이 0인 값이 최대값인지 최소값인지 알 수 없다.
{: .prompt-tip}


### [2.4] Logistic Hypothesis & Cost function
---
지금까지 위에서 다룬 것처럼 직선의 방정식으로 바꾸는 기계 학습을 $\text{Regression}$ (회귀) 라고 한다. 이 수업에서 다루는 내용은 $\text{Classification}$ (분류) 이다. 예를 들어 스팸 메일을 넣었을 때 이게 스팸인지($1$) 아닌지($0$)를 판단하는 문제를 다루는 것이다. 우리가 다룰 문제는 직선을 찾는 것이 아니다! 분류 문제는 $\text{logistic hypothesis}$를 가지고 문제를 풀어야 한다.

#### Logistic Hypothesis
![2-2](assets/img/school_ml/ml2-2.png){: .w-50 .right} 

Sigmoid 함수를 사용한 Hypothesis를 세우면 우리가 원하는 분류 문제를 풀 수 있다. 

$$ \therefore H(X) = \frac{1}{1 + e^{-W^T X}}$$

이 Logistic hypothesis에서 정답에 가깝게 하는 $W$를 찾는 것이 목표이다. 근데 $H(X)$가 이렇게 바뀌면 기존에 사용하던 $\text{Mean Square Error}$를 사용하지 못하고, 새로운 Cost function을 사용해야 한다.

> **H(X)가 Sigmoid 함수로 바뀌면 $\text{Mean Square Error}$를 사용하지 못하는 이유** <br />
Global minimum을 찾아야 하는데, 이 $H(X)$에서는 여러 극값이 생겨 Local minimum에 빠질 수 있기 때문이다. 
{: .prompt-danger}

#### Binary Cross Entropy (B.C.E) Loss

$H(X)$와 정답의 오차가 가져야할 조건을 생각해보자. 
- 정답($y$)이 $1$인 경우: $H(X)$가 $1$로 정답을 맞추면 Cost는 $0$이 되고, $0$으로 오답이면 Cost는 무한대가 된다.
- 정답($y$)이 $0$인 경우: $H(X)$가 $0$으로 정답을 맞추면 Cost는 $0$이 되고, $1$로 오답이면 Cost는 무한대가 된다.

위 두가지 경우를 합친 식이 아래의 $\textcolor{red}{\textbf{Binary Cross Entropy}}$ 이다.

$$
c(H(x), y) = -y \log(H(x)) - (1 - y) \log(1 - H(x))
$$

이 오차를 이용하여 cost function을 아래와 같이 만들고, 이를 통해 경사하강법을 똑같이 진행하면 된다.

$$
Cost(W) = -\frac{1}{m} \sum y \log(H(x)) + (1 - y) \log(1 - H(x))
$$

$$
\therefore W := W - \alpha cost(W)
$$

### 경사하강법 알고리즘 파이썬 실습 
------------
이번 실습의 목표는 데이터 세트가 주어졌을 때 $W$를 랜덤하게 준 상태에서 경사하강법 알고리즘을 이용해 정답에 수렴하게 만드는 것이다. 실습에서 사용하는 데이터 세트는 다음과 같다.

```text
# train.txt
1 1
2 2
3 3
```

#### 데이터 읽기 함수
```python
import numpy as np
import torch

# 데이터 읽기 함수
def load_dataset(file):

  data = np.loadtxt(file)
  print("DATA=",data)
  
  x = data[:,0:-1]
  print("X=",x)
  
  y = np.reshape(data[:,-1],(3,1))
  print("Y=",y)
 
  x = torch.tensor(x, dtype=torch.float)
  y = torch.tensor(y, dtype=torch.float)

  return (x, y)

x, y = load_dataset("/gdrive/My Drive/colab/train.txt")
```
- 배열과 텐서를 다루기 위해 numpy와 torch 라이브러리를 사용해야 한다.
- `x = data[:,0:-1]`는 가장 마지막 열을 제외한 모든 행렬을 가져오겠다는 의미이다. 마지막 열은 정답열이므로 제외한다.
- `y = np.reshape(data[:,-1],(3,1))`는 가장 마지막 열을 가져오는데, 현재 $1\times3$인 행렬을 $3\times1$ 크기로 변경해서 가져오겠다는 의미이다.
- `x`와 `y`를 모두 numpy 배열에서 tensor로 변경하기 위해 `torch.tensor(x, dtype=torch.float)`와 같이 작성한다.
- 더 이상 값을 변경하면 안되는 데이터이므로 x와 y를 튜플 형태로 반환한다.

#### 경사하강법 알고리즘
```python
# 텐서를 리스트로 바꾸는 함수
def tensor2list(input_tensor):
  return input_tensor.cpu().detach().numpy().tolist()

# 가중치 초기화
w = torch.randn(1)*3.0
print("\nInit: w = {0}\n".format(tensor2list(w)))

# 학습률
alpha = 0.1

# 학습데이터 수
m = y.size(0)

# 50회 경사하강 반복
for epoch in range(50):
  cost = 0

  for i in range(m):
    cost = cost + ((w * x[i]) - y[i]) * x[i]
  cost = 1/m * cost
  w = w - alpha * cost

  print("Epoch {0:d}: w = {1}".format(epoch+1,tensor2list(w)))
```
- `torch.randn(1)`은 정규분포 상에서 랜덤으로 float 숫자를 뽑는 것이다. $-3 \sim 3$ 사이의 수가 나올 확률이 $99.5%$ 이상이다. 이 숫자에 3배를 한 값을 $w$로 정하면 $-9$에서 $9$ 사이의 수가 나오게 된다.
  - 만약 $x$가 일차원이 아닌 이차원 이상일 경우, $w$도 고차원 벡터로 정해서 추후에 내적으로 계산을 해야한다.
- `epoch`는 데이터 전체를 한 번 스캔한 것을 의미한다. 
  - 데이터 사이즈가 너무 커질경우, 배치 경사 하강법을 이용한다. 이는 데이터를 랜덤으로 일부만 뽑아내서 읽는 것으로, 한 배치를 다 읽는 것을 `step`이라고 한다.

#### 결과
```text
DATA= [[1. 1.]
 [3. 3.]
 [5. 5.]]
X= [[1.]
 [3.]
 [5.]]
Y= [[1.]
 [3.]
 [5.]]

Init: w = [2.517294406890869]

Epoch 1: w = [0.7471176385879517]
Epoch 2: w = [1.042147159576416]
Epoch 3: w = [0.9929754734039307]
Epoch 4: w = [1.0011707544326782]
Epoch 5: w = [0.9998048543930054]
Epoch 6: w = [1.0000325441360474]
Epoch 7: w = [0.9999945759773254]
Epoch 8: w = [1.0000009536743164]
Epoch 9: w = [0.9999998211860657]
Epoch 10: w = [1.0]
Epoch 11: w = [1.0]
Epoch 12: w = [1.0]
```
- $w$값은 처음에 2.5였지만, 약 10회의 시행만에 1.0으로 값이 수렴하였다.